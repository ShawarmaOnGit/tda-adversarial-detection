{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4867b488",
   "metadata": {},
   "source": [
    "# TDA Analysis\n",
    "\n",
    "**Goal:** Compute persistent homology for clean CIFAR-10 features and visualize topological structure\n",
    "\n",
    "### Overview\n",
    "In this notebook, we will:\n",
    "1. Load the clean features (50-dim PCA-reduced)\n",
    "2. Compute persistence diagrams for H0 (connected components) and H1 (loops)\n",
    "3. Visualize persistence diagrams\n",
    "4. Analyze Betti curves\n",
    "5. Extract topological statistics\n",
    "6. Save diagrams for later comparison with adversarial features\n",
    "\n",
    "---\n",
    "\n",
    "### Why TDA?\n",
    "Traditional ML looks at distances and gradients in feature space. TDA reveals the shape of the data:\n",
    "- **H0 (homology dim 0):** Connected components: how clustered is the data?\n",
    "- **H1 (homology dim 1):** Loops/cycles: are there circular structures?\n",
    "\n",
    "**Our hypothesis:** Adversarial perturbations will fragment clusters (more H0 features) and **destroy** loops (fewer H1 features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dc1df4",
   "metadata": {},
   "source": [
    "### Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8c2867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported everything\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.models.feature_extractor import FeatureExtractor\n",
    "from src.tda.persistence import compute_persistence, save_diagrams, get_persistence_stats\n",
    "from src.utils.plotting import plot_persistence_diagram, plot_betti_curve\n",
    "\n",
    "np.random.seed(111)\n",
    "\n",
    "print(\"Successfully imported everything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7396fb",
   "metadata": {},
   "source": [
    "### Load Clean Features\n",
    "\n",
    "We'll use the 50-dimensional PCA-reduced features from `02_feature_extraction.ipynb`. These features were extracted from 45,000 clean CIFAR-10 training images using ResNet50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8dadefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean features shape: (45000, 50)\n",
      "Feature range: [-2.4786, 1.6122]\n",
      "Labels: (45000,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('../results/features/clean_features_reduced.npz')\n",
    "clean_features = data['features']\n",
    "clean_labels = data['labels']\n",
    "\n",
    "print(f\"Clean features shape: {clean_features.shape}\")\n",
    "print(f\"Feature range: [{clean_features.min():.4f}, {clean_features.max():.4f}]\")\n",
    "print(f\"Labels: {clean_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485379fd",
   "metadata": {},
   "source": [
    "### Subsample for TDA\n",
    "\n",
    "Computing persistent homology on 45,000 points is computationally expensive. Therefore, let's use a subset for initial analysis:\n",
    "- **10,000 samples** for quick experimentation\n",
    "- Later we can scale up to full dataset if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b9e18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using 10000 samples for TDA\n",
      "Subset shape: (10000, 50)\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10000\n",
    "indices = np.random.choice(len(clean_features), n_samples, replace=False)\n",
    "clean_features_subset = clean_features[indices]\n",
    "clean_labels_subset = clean_labels[indices]\n",
    "\n",
    "print(f\"\\nUsing {n_samples} samples for TDA\")\n",
    "print(f\"Subset shape: {clean_features_subset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aa2767",
   "metadata": {},
   "source": [
    "### Compute Persistent Homology\n",
    "\n",
    "We'll compute:\n",
    "- **H0:** Connected components (clusters)\n",
    "- **H1:** 1-dimensional loops (cycles in the data)\n",
    "\n",
    "This will 1-2 mins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c8a58dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing persistence for 10000 points in 50D:\n",
      "Persistence computed\n",
      "H0 features: 10000\n",
      "H1 features: 14978\n",
      "\n",
      "Persistence computation successfully completed\n"
     ]
    }
   ],
   "source": [
    "result_clean = compute_persistence(\n",
    "    clean_features_subset,\n",
    "    maxdim=1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nPersistence computation successfully completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5dcd6d",
   "metadata": {},
   "source": [
    "### Extract Persistence Diagrams\n",
    "\n",
    "The result contains two diagrams:\n",
    "- `result_clean['dgms'][0]` → H0 diagram (connected components)\n",
    "- `result_clean['dgms'][1]` → H1 diagram (loops)\n",
    "\n",
    "Each diagram is an array of (birth, death) pairs. Let's extract the diagrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6eff566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H0 diagram: 10000 features\n",
      "H1 diagram: 14978 features\n",
      "\n",
      "--- H0 Statistics (Connected Components) ---\n",
      "Number of features: 9999\n",
      "Total persistence: 2831.9765\n",
      "Mean persistence: 0.2832\n",
      "Max persistence: 1.1873\n",
      "\n",
      "--- H1 Statistics (Loops) ---\n",
      "Number of features: 14978\n",
      "Total persistence: 161.5036\n",
      "Mean persistence: 0.0108\n",
      "Max persistence: 0.0976\n"
     ]
    }
   ],
   "source": [
    "clean_h0 = result_clean['dgms'][0]\n",
    "clean_h1 = result_clean['dgms'][1]\n",
    "\n",
    "print(f\"H0 diagram: {len(clean_h0)} features\")\n",
    "print(f\"H1 diagram: {len(clean_h1)} features\")\n",
    "\n",
    "print(\"\\n--- H0 Statistics (Connected Components) ---\")\n",
    "h0_stats = get_persistence_stats(clean_h0)\n",
    "print(f\"Number of features: {h0_stats['n_features']}\")\n",
    "print(f\"Total persistence: {h0_stats['total_persistence']:.4f}\")\n",
    "print(f\"Mean persistence: {h0_stats['mean_persistence']:.4f}\")\n",
    "print(f\"Max persistence: {h0_stats['max_persistence']:.4f}\")\n",
    "\n",
    "print(\"\\n--- H1 Statistics (Loops) ---\")\n",
    "h1_stats = get_persistence_stats(clean_h1)\n",
    "print(f\"Number of features: {h1_stats['n_features']}\")\n",
    "print(f\"Total persistence: {h1_stats['total_persistence']:.4f}\")\n",
    "print(f\"Mean persistence: {h1_stats['mean_persistence']:.4f}\")\n",
    "print(f\"Max persistence: {h1_stats['max_persistence']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f09aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
